# MLflow Model System - Customer Churn Prediction

A production-ready MLOps system for training, evaluating, and managing XGBoost models for customer churn prediction using MLflow.

## Overview

This project implements a complete MLOps pipeline for customer churn prediction using:

- **MLflow** for experiment tracking, model registry, and artifact management
- **XGBoost** for high-performance gradient boosting classification
- **MinIO** for S3-compatible artifact storage
- **Docker** for containerized MLflow server
- **SHAP** for model explainability

The system provides end-to-end functionality from data preprocessing to model deployment with full tracking and versioning capabilities.

## Features
### Experiment Tracking
- Automatic logging of parameters, metrics, and artifacts
- Support for nested runs and experiment organization
- Integration with MLflow UI for visualization

### Model Training
- XGBoost binary classification with GPU support
- Configurable hyperparameters via YAML
- Automatic feature importance logging
- Early stopping and validation monitoring

### Model Evaluation
- Comprehensive metrics (AUC, log loss, accuracy)
- SHAP explainability integration
- Model comparison against baseline
- Threshold validation for deployment gates

### Model Registry
- Version control for trained models
- Alias management (staging, champion, production)
- Model promotion workflows
- Complete model lifecycle tracking

### Infrastructure
- Dockerized MLflow tracking server
- MinIO for artifact storage (S3-compatible)
- PostgreSQL backend for metadata
- Production-ready configuration

## ğŸ“ Project Structure

```
model_pipeline/
â”œâ”€â”€ docker-compose.yaml          # MLflow infrastructure setup
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ test_end_to_end.ipynb   # End-to-end testing notebook
â””â”€â”€ src/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.yaml          # Model and MLflow configuration
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ train.csv            # Training dataset
    â”‚   â”œâ”€â”€ test.csv             # Testing dataset
    â”‚   â””â”€â”€ *.ipynb              # Data preprocessing notebooks
    â”œâ”€â”€ mlflow_utils/
    â”‚   â”œâ”€â”€ experiment_tracker.py    # MLflow experiment management
    â”‚   â””â”€â”€ model_registry.py        # Model registry operations
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ xgboost_trainer.py      # XGBoost training pipeline
    â”‚   â””â”€â”€ evaluator.py            # Model evaluation logic
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ train.py                # Training script
    â”‚   â”œâ”€â”€ eval.py                 # Evaluation script
    â”‚   â””â”€â”€ register_model.py       # Model registry CLI
    â”œâ”€â”€ run_sh/
    â”‚   â”œâ”€â”€ train.sh                # Training execution script
    â”‚   â”œâ”€â”€ eval.sh                 # Evaluation execution script
    â”‚   â”œâ”€â”€ register_model.sh       # Model registration script
    â”‚   â”œâ”€â”€ set_model_alias.sh      # Set model alias
    â”‚   â”œâ”€â”€ promote_model.sh        # Promote model to production
    â”‚   â”œâ”€â”€ list_models.sh          # List registered models
    â”‚   â””â”€â”€ model_info.sh           # Get model information
    â””â”€â”€ utility/
        â””â”€â”€ helper.py               # Utility functions
```

## Prerequisites

- **Python**: 3.10 or higher
- **Docker & Docker Compose**: For MLflow infrastructure
- **CUDA** (optional): For GPU-accelerated training
- **Git**: For version control



## âš™ï¸ Configuration

### Main Configuration File: `src/config/config.yaml`

```yaml
mlflow:
  tracking_uri: "http://localhost:5000"
  experiment_name: "test_churn_prediction_v0.1"
  artifact_location: "s3://mlflow/"
  registry_uri: "http://localhost:5000"
  tags:
    task: "churn_prediction"
    purpose: "test"

model:
  name: "xgboost_churn"
  version: "0.1.0"
  type: "classifier"
  train_test_split: 0.2
  random_state: 42
  
  xgboost:
    booster: "gbtree"
    device: "cuda"  # or "cpu"
    max_depth: 6
    eta: 0.1
    objective: "binary:logistic"
    eval_metric: ["auc", "logloss", "error@0.5"]
    # ... more parameters

evaluation:
  shap:
    enabled: true
    explainer_type: "exact"
    max_samples: 100
  
  thresholds:
    accuracy: 0.85
    auc: 0.80

features:
  target_column: "Churn"
  training_features:
    - Age
    - Tenure
    - Usage Frequency
    # ... more features
```

### Environment Variables

The system uses these environment variables (set in training/eval scripts):

```bash
export AWS_ACCESS_KEY_ID="minio"
export AWS_SECRET_ACCESS_KEY="minio123"
export AWS_DEFAULT_REGION="us-east-1"
export MLFLOW_S3_ENDPOINT_URL="http://localhost:9000"
```

## ğŸ¬ Quick Start

### 1. Train a Model

```bash
cd src/run_sh
chmod +x *.sh

# Run training
./train.sh
```

**Output**: You'll get a `run_id` (e.g., `20c0e794e88f41ab9fe3685a06c54874`)

### 2. Evaluate the Model

```bash
# Update RUN_ID in eval.sh with your run_id
./eval.sh
```

### 3. Register the Model

```bash
# Update RUN_ID in register_model.sh
./register_model.sh
```

### 4. Promote to Production

```bash
# Update VERSION in set_model_alias.sh
./set_model_alias.sh

# Promote to champion
./promote_model.sh
```

### 5. Use the Model

```python
import mlflow

# Load production model
model = mlflow.pyfunc.load_model("models:/xgboost_churn_model@champion")

# Make predictions
predictions = model.predict(test_data)
```

## ğŸ”„ Model Registry Workflow

### Complete Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Training  â”‚  ./train.sh
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluation â”‚  ./eval.sh
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Registrationâ”‚  ./register_model.sh
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Staging   â”‚  ./set_model_alias.sh (alias=staging)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Validation â”‚  Additional testing
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Production â”‚  ./promote_model.sh
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Best Practices

1. **Development Cycle**
   ```bash
   # Train multiple models
   ./train.sh  # experiment-1
   ./train.sh  # experiment-2
   ./train.sh  # experiment-3
   
   # Evaluate all
   ./eval.sh  # Update RUN_ID each time
   
   # Register best model
   ./register_model.sh
   ```

2. **Staging Validation**
   ```bash
   # Register and set to staging
   ./register_model.sh
   ./set_model_alias.sh  # alias=staging
   
   # Test in staging environment
   # Run A/B tests
   # Validate performance
   
   # Promote when ready
   ./promote_model.sh
   ```

3. **Production Deployment**
   ```bash
   # Always compare against current champion
   python src/scripts/eval.py \
       --model-uri "models:/xgboost_churn_model@staging" \
       --compare-baseline "models:/xgboost_churn_model@champion"
   
   # If improvement confirmed, promote
   ./promote_model.sh
   ```

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MLflow UI (Port 5000)                â”‚
â”‚                   Experiment Tracking & Registry         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL  â”‚ â”‚    MinIO     â”‚ â”‚Training/Eval â”‚
â”‚   Metadata   â”‚ â”‚  Artifacts   â”‚ â”‚   Scripts    â”‚
â”‚   Storage    â”‚ â”‚  (S3-like)   â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
Input Data â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Registration â†’ Production
    â†“            â†“              â†“           â†“            â†“             â†“
  Raw CSV    Cleaned Data   XGBoost    Metrics     Model URI    Deployed Model
                                       SHAP         Version
```

### Module Architecture

```
mlflow_utils/
â”œâ”€â”€ experiment_tracker.py   â†’ Handles run lifecycle, logging
â””â”€â”€ model_registry.py       â†’ Model versioning, aliases

model/
â”œâ”€â”€ xgboost_trainer.py     â†’ Training pipeline, feature engineering
â””â”€â”€ evaluator.py          â†’ Evaluation metrics, SHAP

scripts/
â”œâ”€â”€ train.py              â†’ CLI for training
â”œâ”€â”€ eval.py               â†’ CLI for evaluation
â””â”€â”€ register_model.py     â†’ CLI for registry operations
```